{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wiki_dump_reader import Cleaner , iterate\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def calculate_time(func):\n",
    "    \"\"\"\n",
    "    A decorator function to calculate the execution time of another function.\n",
    "\n",
    "    Args:\n",
    "        func (function): The function whose execution time is to be measured.\n",
    "\n",
    "    Returns:\n",
    "        function: A wrapped function that calculates and prints the execution time of the original function.\n",
    "\n",
    "    \"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        \"\"\"\n",
    "        Calculate the execution time of the decorated function.\n",
    "\n",
    "        Args:\n",
    "            *args: Positional arguments to be passed to the decorated function.\n",
    "            **kwargs: Keyword arguments to be passed to the decorated function.\n",
    "\n",
    "        Returns:\n",
    "            Any: The result of the decorated function.\n",
    "        \n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        print(f\"Time taken by {func.__name__}: {end_time - start_time} seconds\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def memory_location(func):\n",
    "    \"\"\"\n",
    "    A decorator function to print the memory location of a wrapped function.\n",
    "\n",
    "    Args:\n",
    "        func (function): The function whose memory location is to be printed.\n",
    "\n",
    "    Returns:\n",
    "        function: A wrapped function that prints the memory location of the original function and then calls it.\n",
    "\n",
    "    \"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        \"\"\"\n",
    "        Print the memory location of the decorated function and then call it.\n",
    "\n",
    "        Args:\n",
    "            *args: Positional arguments to be passed to the decorated function.\n",
    "            **kwargs: Keyword arguments to be passed to the decorated function.\n",
    "\n",
    "        Returns:\n",
    "            Any: The result of the decorated function.\n",
    "        \n",
    "        \"\"\"\n",
    "        print(f\"Memory location of {func.__name__}: {id(func)}\")\n",
    "        return func(*args, **kwargs)\n",
    "    return wrapper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corpus Create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@calculate_time\n",
    "@memory_location\n",
    "def create_corpus():\n",
    "    \"\"\"\n",
    "    Create a corpus from a Wikipedia dump file in Hindi.\n",
    "\n",
    "    This function iterates over pages in a file named 'hiwiki-latest-pages-articles.xml',\n",
    "    cleans the text, and writes it along with the title to a new file named 'Hindi_Corpus.txt'.\n",
    "    It also updates a progress bar using tqdm to show the processing progress.\n",
    "    \n",
    "    Args:\n",
    "        None\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Raises:\n",
    "        None\n",
    "    \n",
    "    \"\"\"\n",
    "    corpus_file = 'Hindi_Corpus.txt'\n",
    "    corpus_limit = 232729\n",
    "    page_count = 0\n",
    "    cleaner = Cleaner()  # Assuming Cleaner class is imported or defined elsewhere.\n",
    "    \n",
    "    # Open the output file for writing\n",
    "    with open(corpus_file, 'w', encoding='utf-8') as output:\n",
    "        pg_bar = tqdm(total=corpus_limit)  # Initialize the progress bar\n",
    "        # Iterate over pages in 'hiwiki-latest-pages-articles.xml'\n",
    "        for title, text in iterate('hiwiki-latest-pages-articles.xml'):  # Assuming iterate function is defined elsewhere.\n",
    "            # Clean the text\n",
    "            text = cleaner.clean_text(text)\n",
    "            cleaned_text, _ = cleaner.build_links(text)\n",
    "            # Write title and cleaned text to the corpus file\n",
    "            output.write(title + '\\n' + cleaned_text + '\\n')\n",
    "            page_count += 1\n",
    "            if page_count % 1000 == 0:\n",
    "                pg_bar.update(1000)  # Update progress bar every 1000 pages\n",
    "        pg_bar.close()  # Close the progress bar\n",
    "        output.close()  # Close the output file\n",
    "    print(f\"\\nPage count = {page_count}\")  # Print total page count after processing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stop_words():\n",
    "    \"\"\"\n",
    "    Create a list of stop words from a file named 'stopwords.txt'.\n",
    "\n",
    "    This function reads the stop words from the file, strips newline characters,\n",
    "    and splits the words by spaces to create a list of stop words.\n",
    "\n",
    "    Args:\n",
    "        None\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of stop words.\n",
    "    \n",
    "    Raises:\n",
    "        None\n",
    "    \n",
    "    \"\"\"\n",
    "    with open('stopwords.txt', 'r', encoding='utf-8') as stop:\n",
    "        y = stop.readlines()\n",
    "    x = []\n",
    "    for element in y:\n",
    "        element = element.strip('\\n')\n",
    "        x.extend(element.split(' '))\n",
    "    return x\n",
    "\n",
    "\n",
    "stop_words=create_stop_words()\n",
    "\n",
    "def remove_stop_words(string):\n",
    "    \"\"\"\n",
    "    Remove stop words from a given string.\n",
    "\n",
    "    This function takes a string as input, splits it into words, and removes\n",
    "    any words that are found in a predefined list of stop words. It then joins\n",
    "    the remaining words back into a single string and returns it.\n",
    "\n",
    "    Args:\n",
    "        string (str): The input string from which stop words are to be removed.\n",
    "\n",
    "    Returns:\n",
    "        str: The input string with stop words removed.\n",
    "    \n",
    "    Raises:\n",
    "        None\n",
    "    \n",
    "    \"\"\"\n",
    "    l = string.split()  # Split the input string into a list of words\n",
    "    return_list = []\n",
    "    for x in l:\n",
    "        if x not in stop_words:  # Check if the word is not in the stop_words list\n",
    "            return_list.append(x)  # If not, add it to the return_list\n",
    "    return ' '.join(return_list)  # Join the words in return_list back into a string and return it\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "def remove_foreign(x):\n",
    "    \"\"\"\n",
    "    Remove foreign characters from a given string.\n",
    "\n",
    "    This function takes a string as input and removes any characters that are not\n",
    "    part of the Devanagari script, which is commonly used for writing languages like Hindi, Sanskrit, etc.\n",
    "    \n",
    "    Args:\n",
    "        x (str): The input string from which foreign characters are to be removed.\n",
    "\n",
    "    Returns:\n",
    "        str: The input string with foreign characters removed.\n",
    "    \n",
    "    Raises:\n",
    "        None\n",
    "    \n",
    "    \"\"\"\n",
    "    string = x.split(' ')  # Split the input string into a list of words\n",
    "    y = [(re.compile(r'[\\u0901-\\u0939\\u093C-\\u094D\\u0950-\\u0954\\u0958-\\u0963\\u097B-\\u097F]')).findall(s) for s in string]\n",
    "    z = [''.join(s) for s in y]  # Join the characters found in each word back into a string\n",
    "    w = ' '.join(z)  # Join the resulting strings back into a single string\n",
    "    return w  # Return the string with foreign characters removed\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "@calculate_time\n",
    "@memory_location\n",
    "def pre(source, destination):\n",
    "    \"\"\"\n",
    "    Preprocess a source text file and save the result to a destination file.\n",
    "\n",
    "    This function reads text from the source file, removes foreign characters\n",
    "    and stop words, and writes the preprocessed text to the destination file.\n",
    "    It also updates a progress bar to show the processing progress.\n",
    "\n",
    "    Args:\n",
    "        source (str): The path to the source text file.\n",
    "        destination (str): The path to save the preprocessed text file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Raises:\n",
    "        None\n",
    "\n",
    "    \"\"\"\n",
    "    line_count = 0\n",
    "    with open(source, 'r', encoding='utf-8') as input:\n",
    "        with open(destination, 'w', encoding='utf-8') as output:\n",
    "            bar = tqdm(total=5000000)  # Assuming 5000000 is the total number of lines\n",
    "            for line in input:\n",
    "                string = line.replace('\\n', '')\n",
    "                string = remove_foreign(string)\n",
    "                string = remove_stop_words(string)\n",
    "                output.write(string)\n",
    "                line_count += 1\n",
    "                if line_count % 10000 == 0:\n",
    "                    bar.update(10000)  # Update progress bar every 10000 lines\n",
    "            bar.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre('Hindi_Corpus.txt', 'PreProcessed_Corpus.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working on reduced Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize_pre(so,de):\n",
    "    i=10\n",
    "    with open(so, 'r',encoding=\"utf-8\") as sor:\n",
    "        with open(de, 'w',encoding=\"utf-8\") as des:\n",
    "                for line in sor:\n",
    "                    des.write(line[:(len(line))//i])\n",
    "\n",
    "minimize_pre(\"PreProcessed_Corpus - Copy.txt\", \"PreProcessed_Corpus.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "const_vocab=100\n",
    "\n",
    "def vocab():\n",
    "    \"\"\"\n",
    "    Reads the text from 'trialfile.txt' and returns it as a list of words.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of words extracted from the file 'trialfile.txt'.\n",
    "    \n",
    "    \"\"\"\n",
    "    with open('PreProcessed_Corpus.txt', 'r', encoding='utf-8') as r:\n",
    "        text = r.read().split()\n",
    "    word_counts = Counter(text)\n",
    "    text = [word for word in tqdm(text) if word_counts[word] > const_vocab]  \n",
    "    return text\n",
    "\n",
    "def gen_distinct_vocab():\n",
    "    \"\"\"\n",
    "    Reads the text from 'trialfile.txt', extracts distinct words, and returns them as a list.\n",
    "\n",
    "    This function computes the vocabulary size and stores it in a global variable 'vocabulary_size'.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of distinct words extracted from the file 'trialfile.txt'.\n",
    "    \n",
    "    \"\"\"\n",
    "    with open('PreProcessed_Corpus.txt', 'r', encoding='utf-8') as r:\n",
    "        text = r.read().split()\n",
    "        counter = Counter(text)\n",
    "        reduced_vocab = [item for item, count in counter.items() if count > const_vocab]\n",
    "        text_set = set(reduced_vocab)\n",
    "        global vocabulary_size\n",
    "        vocabulary_size = len(text_set)\n",
    "    return list(text_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "global words_list\n",
    "words_list=vocab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing total number of tokens and vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_vocab=gen_distinct_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = len(distinct_vocab)\n",
    "print((vocabulary_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from nltk import ngrams, word_tokenize\n",
    "\n",
    "def gen_grams():            \n",
    "    global tokens\n",
    "    global grams\n",
    "    tokens = word_tokenize(' '.join(words_list))\n",
    "    grams = ngrams(tokens, 6)\n",
    "    return None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_grams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of tokens is: \", len(tokens))\n",
    "print(\"Size of the vocabulary: \", vocabulary_size)\n",
    "print(\"First 1500 words of the Vocabulary: \", words_list[:1500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np \n",
    "\n",
    "matrix = np.zeros((len(distinct_vocab),len(distinct_vocab)),dtype=np.int64)\n",
    "\n",
    "@calculate_time\n",
    "@memory_location\n",
    "def create_matrix():\n",
    "\n",
    "    for x in tqdm(grams):\n",
    "    \n",
    "        \n",
    "        if x[0] in distinct_vocab:\n",
    "            if x[1] in distinct_vocab:\n",
    "                matrix[distinct_vocab.index(x[0])][distinct_vocab.index(x[1])] += 5\n",
    "            if x[2] in distinct_vocab:\n",
    "                matrix[distinct_vocab.index(x[0])][distinct_vocab.index(x[2])] += 4\n",
    "            if x[3] in distinct_vocab:\n",
    "                matrix[distinct_vocab.index(x[0])][distinct_vocab.index(x[3])] += 3\n",
    "            if x[4] in distinct_vocab:\n",
    "                matrix[distinct_vocab.index(x[0])][distinct_vocab.index(x[4])] += 2\n",
    "            if x[5] in distinct_vocab:\n",
    "                matrix[distinct_vocab.index(x[0])][distinct_vocab.index(x[5])] += 1\n",
    "        if x[5] in distinct_vocab:\n",
    "            if x[1] in distinct_vocab:\n",
    "                matrix[distinct_vocab.index(x[5])][distinct_vocab.index(x[1])] += 2\n",
    "            if x[2] in distinct_vocab:\n",
    "                matrix[distinct_vocab.index(x[5])][distinct_vocab.index(x[2])] += 3\n",
    "            if x[3] in distinct_vocab:\n",
    "                matrix[distinct_vocab.index(x[5])][distinct_vocab.index(x[3])] += 4\n",
    "            if x[4] in distinct_vocab:\n",
    "                matrix[distinct_vocab.index(x[5])][distinct_vocab.index(x[4])] += 5\n",
    "            if x[0] in distinct_vocab:\n",
    "                matrix[distinct_vocab.index(x[5])][distinct_vocab.index(x[0])] += 1\n",
    "\n",
    "        \n",
    "create_matrix()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "@calculate_time\n",
    "@memory_location\n",
    "def calculate_probability(matrix):\n",
    "    \n",
    "    # Calculate row sums and column sums\n",
    "    row_sums = np.sum(matrix, axis=1)\n",
    "    col_sums = np.sum(matrix, axis=0)\n",
    "\n",
    "    # Total co-occurrences\n",
    "    N = np.sum(matrix)\n",
    "\n",
    "    if(N==0):\n",
    "        return 0,0\n",
    "\n",
    "    # Calculate pi and pj\n",
    "    pI = row_sums / N\n",
    "    pJ = col_sums / N\n",
    "\n",
    "    global pi,pj\n",
    "\n",
    "    pi,pj=pI,pJ\n",
    " \n",
    " \n",
    "@calculate_time\n",
    "@memory_location\n",
    "def gen_ppmi_matrix(matrix):\n",
    "    co_occurrence_matrix=matrix\n",
    " \n",
    "    ppmi = np.zeros((len(distinct_vocab), len(distinct_vocab)))\n",
    "\n",
    "    row_sums = np.sum(co_occurrence_matrix, axis=1)\n",
    "    for i in tqdm(range(len(distinct_vocab))):\n",
    "        for j in range(len(distinct_vocab)):\n",
    "\n",
    "            if (row_sums[i] == 0 or co_occurrence_matrix[i][j] == 0):\n",
    "                ppmi[i][j] = 0\n",
    "                continue\n",
    "            pij = ((co_occurrence_matrix[i][j]))/row_sums[i]\n",
    "            ppmi[i][j] = max(0, math.log2(pij/(pi[i]*pj[j])))\n",
    "\n",
    "    return ppmi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_probability(matrix)\n",
    "ppmi_matrix=gen_ppmi_matrix(matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding 10 most common nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "my_list = vocab()\n",
    "counter = Counter(my_list)\n",
    "common_elements = counter.most_common(10)\n",
    "most_common_elements=[a for (a,_) in common_elements]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing and printing the top ten nearest neighbour dictionary. (Without Multiprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def insert_into_sorted_list(sorted_list, element):\n",
    "    index = 0\n",
    "    while index < len(sorted_list) and sorted_list[index][1] > element[1]:\n",
    "        index += 1\n",
    "    sorted_list.insert(index, element)\n",
    "    return sorted_list[:-1]\n",
    "\n",
    "def cosine(a,b):\n",
    "        mag_a = math.sqrt(sum(component ** 2 for component in a))\n",
    "        mag_b = math.sqrt(sum(component ** 2 for component in b))\n",
    "        dot_product = sum(ai * bi for ai, bi in zip(a, b))\n",
    "        if mag_a == 0 or mag_b ==0:\n",
    "            return 0\n",
    "        else:\n",
    "            return dot_product/(mag_a*mag_b)\n",
    "\n",
    "def find_nearest_neighbor_of_noun(index):\n",
    "    l=[(distinct_vocab[0],(cosine(ppmi_matrix[index], ppmi_matrix[0]))) for _ in range(11)]\n",
    "    for i in tqdm(range(1,len(distinct_vocab))):\n",
    "        if i!=index:\n",
    "            if cosine(ppmi_matrix[index],ppmi_matrix[i])>l[10][1]:\n",
    "                l=insert_into_sorted_list(l,(distinct_vocab[i],(cosine(ppmi_matrix[index], ppmi_matrix[i]))))\n",
    "    return l\n",
    "\n",
    "nearest_neighbour_dict={}\n",
    "\n",
    "@calculate_time\n",
    "@memory_location\n",
    "def nearest_seq():\n",
    "    for x in tqdm(most_common_elements):\n",
    "        nearest_neighbour_dict[x]=find_nearest_neighbor_of_noun(distinct_vocab.index(x))\n",
    "\n",
    "nearest_seq()\n",
    "\n",
    "print(nearest_neighbour_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above output cell shows the time taken by the sequential process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment 6 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing Co-occurrence matrix with Multiprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "\n",
    "n = vocabulary_size\n",
    "p_mat = multiprocessing.Array('d', n*n)\n",
    "\n",
    "def compile_grams(x,p_mat):\n",
    "    leng=len(distinct_vocab)\n",
    "    \n",
    "\n",
    "    if x[0] in distinct_vocab:\n",
    "        if x[1] in distinct_vocab:\n",
    "            p_mat[distinct_vocab.index(x[0])*leng+distinct_vocab.index(x[1])] += 5\n",
    "        if x[2] in distinct_vocab:\n",
    "            p_mat[distinct_vocab.index(x[0])*leng+distinct_vocab.index(x[2])] += 4\n",
    "        if x[3] in distinct_vocab:\n",
    "            p_mat[distinct_vocab.index(x[0])*leng+distinct_vocab.index(x[3])] += 3\n",
    "        if x[4] in distinct_vocab:\n",
    "            p_mat[distinct_vocab.index(x[0])*leng+distinct_vocab.index(x[4])] += 2\n",
    "        if x[5] in distinct_vocab:\n",
    "            p_mat[distinct_vocab.index(x[0])*leng+distinct_vocab.index(x[5])] += 1\n",
    "    if x[5] in distinct_vocab:\n",
    "        if x[1] in distinct_vocab:\n",
    "            p_mat[distinct_vocab.index(x[5])*leng+distinct_vocab.index(x[1])] += 2\n",
    "        if x[2] in distinct_vocab:\n",
    "            p_mat[distinct_vocab.index(x[5])*leng+distinct_vocab.index(x[2])] += 3\n",
    "        if x[3] in distinct_vocab:\n",
    "            p_mat[distinct_vocab.index(x[5])*leng+distinct_vocab.index(x[3])] += 4\n",
    "        if x[4] in distinct_vocab:\n",
    "            p_mat[distinct_vocab.index(x[5])*leng+distinct_vocab.index(x[4])] += 5\n",
    "        if x[0] in distinct_vocab:\n",
    "            p_mat[distinct_vocab.index(x[5])*leng+distinct_vocab.index(x[0])] += 1\n",
    "\n",
    "@calculate_time\n",
    "@memory_location\n",
    "def create_matrix_p(p_mat):\n",
    "    grams = ngrams(tokens,6)\n",
    "\n",
    "    jobs=[]\n",
    "\n",
    "    for x in tqdm(grams):\n",
    "        p=multiprocessing.Process(target=compile_grams, args=(x,p_mat,))\n",
    "        p.start()\n",
    "        jobs.append(p)\n",
    "\n",
    "    for job in jobs:\n",
    "        job.join()\n",
    "\n",
    "create_matrix_p(p_mat)\n",
    "\n",
    "p_matrix = np.zeros((n,n),dtype=np.int64)\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        p_matrix[i][j]=p_mat[i*n+j]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above output cell shows the time taken by multiprocessing to create the co-occurrence matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Multiprocessing to compute the nearest neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import multiprocessing\n",
    "\n",
    "neigh = multiprocessing.Manager().dict()\n",
    "\n",
    "def p_insert_into_sorted_list(sorted_list, element):\n",
    "    index = 0\n",
    "    while index < len(sorted_list) and sorted_list[index][1] > element[1]:\n",
    "        index += 1\n",
    "    sorted_list.insert(index, element)\n",
    "    return sorted_list[:-1]\n",
    "\n",
    "def p_cosine(a,b):\n",
    "        mag_a = math.sqrt(sum(component ** 2 for component in a))\n",
    "        mag_b = math.sqrt(sum(component ** 2 for component in b))\n",
    "        dot_product = sum(ai * bi for ai, bi in zip(a, b))\n",
    "        if mag_a == 0 or mag_b ==0:\n",
    "            return 0\n",
    "        else:\n",
    "            return dot_product/(mag_a*mag_b)\n",
    "\n",
    "def p_find_nearest_neighbor_of_noun(index,neigh):\n",
    "    l=[(distinct_vocab[0],(p_cosine(ppmi_matrix[index], ppmi_matrix[0]))) for _ in range(11)]\n",
    "    for i in tqdm(range(1,n)):\n",
    "        if i!=index:\n",
    "            if p_cosine(ppmi_matrix[index],ppmi_matrix[i])>l[10][1]:\n",
    "                l=p_insert_into_sorted_list(l,(distinct_vocab[i],(p_cosine(ppmi_matrix[index], ppmi_matrix[i]))))\n",
    "\n",
    "    neigh[distinct_vocab[index]]=l\n",
    "\n",
    "@calculate_time\n",
    "@memory_location\n",
    "def parallel_nearest():\n",
    "    jobs=[]\n",
    "    for x in (most_common_elements):\n",
    "        p=multiprocessing.Process(target=p_find_nearest_neighbor_of_noun, args=(distinct_vocab.index(x),neigh,))\n",
    "        p.start()\n",
    "        jobs.append(p)\n",
    "\n",
    "    for job in jobs:\n",
    "        job.join()\n",
    "        \n",
    "parallel_nearest()\n",
    "\n",
    "print(neigh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time taken to create the nearest neighbours dictionary using multiprocessing. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
