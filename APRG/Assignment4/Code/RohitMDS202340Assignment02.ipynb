{"cells":[{"cell_type":"markdown","source":["# Assignment 2 :\n","\n","## Rohit Roy | MDS202340"],"metadata":{"id":"CeyLN-zpT-dX"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"J1M4eiVWmWil","executionInfo":{"status":"ok","timestamp":1697404180220,"user_tz":-330,"elapsed":2608,"user":{"displayName":"Rohit Roy","userId":"13123088003246438727"}}},"outputs":[],"source":["from tqdm import tqdm\n","import regex as re\n","import os\n","import glob\n","from nltk import ngrams, word_tokenize, sent_tokenize,pos_tag\n","import nltk\n","import random\n","from collections import Counter, defaultdict"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9t-0UT4PmWim"},"outputs":[],"source":["# Merging into one file\n","\n","def merge_files():\n","\n","    input_files = glob.glob(os.path.join(\"wiki\", \"*.txt\"))\n","    input_files = sorted(input_files)\n","\n","    output_file = \"process_wiki/wiki.txt\"\n","    with open(output_file, \"w\", encoding=\"utf-8\") as output:\n","        for input_file in input_files:\n","            with open(input_file, \"r\", encoding=\"utf-8\") as input:\n","                # Read the content of the input file and write it to the output file\n","                content = input.read()\n","                output.write(content)\n","\n","merge_files()"]},{"cell_type":"markdown","metadata":{"id":"-Smm8KJgmWin"},"source":["# Part 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DeCR-zaWmWio","outputId":"84fdaa1f-0f53-4144-af51-d5ec1e857d8c"},"outputs":[{"name":"stdout","output_type":"stream","text":["The number of sentences is 7909229\n"]}],"source":["# Counting number of sentences.\n","\n","with open(\"process_wiki/wiki.txt\",\"r\",encoding=\"utf-8\") as input:\n","    content = input.read()\n","    sentences = sent_tokenize(content)\n","\n","num_sent = len(sentences)\n","print(f\"The number of sentences is {num_sent}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lER6IgO8mWip"},"outputs":[],"source":["from collections import defaultdict\n","\n","# Tokenizing the Corpus into words.\n","words = [word for sentence in sentences for word in word_tokenize(sentence)]\n","\n","# Create 4-grams.\n","four_grams = list(ngrams(words, 4))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JV5LYUGumWip"},"outputs":[],"source":["# Now we calculate the probabilities in the list of ngrams.\n","from collections import Counter\n","\n","# We create a nested dictionary of words in the corpus with their probability distributions.\n","ngram_model = defaultdict(Counter)\n","\n","# For every trigram, there is a separate Counter for each 4th word.\n","for w1, w2, w3, w4 in four_grams:\n","    ngram_model[(w1, w2, w3)][w4] += 1\n","\n","# Converting those counts to probabilities.\n","for trigram in ngram_model:\n","    total_count = sum(ngram_model[trigram].values())\n","    for w4 in ngram_model[trigram]:\n","        ngram_model[trigram][w4] = ngram_model[trigram][w4] / total_count"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FXSoalrfmWip","executionInfo":{"status":"ok","timestamp":1697404930235,"user_tz":-330,"elapsed":616,"user":{"displayName":"Rohit Roy","userId":"13123088003246438727"}},"outputId":"0f954e87-4e8e-4624-dd7b-d3a5d74242fb"},"outputs":[{"output_type":"stream","name":"stdout","text":["The next word that is predicted : \n"]},{"output_type":"execute_result","data":{"text/plain":["{('with', 'the', 'help'): 'of',\n"," ('focused', 'on', 'the'): 'development',\n"," ('for', 'the', 'people'): 'of',\n"," ('have', 'ranged', 'from'): 'narrative',\n"," ('to', 'reduce', 'their'): 'ecological'}"]},"metadata":{},"execution_count":14}],"source":["trigram_list = [(\"with\", \"the\", \"help\"), (\"focused\", \"on\", \"the\"), (\"for\", \"the\", \"people\"), (\"have\", \"ranged\", \"from\"), (\"to\", \"reduce\", \"their\")]\n","\n","# Now we predict the next word based on the probabilities.\n","prediction = {}\n","\n","for trigram in trigram_list:\n","    next_word_prob = ngram_model[trigram]\n","    if next_word_prob:\n","        predicted_word = max(next_word_prob, key=next_word_prob.get)\n","    else:\n","        predicted_word = \"<UNSEEN>\"\n","    prediction[trigram] = predicted_word\n","\n","print(\"The next word that is predicted : \")\n","prediction"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Nq73jdT1mWip","executionInfo":{"status":"ok","timestamp":1697406038529,"user_tz":-330,"elapsed":3231,"user":{"displayName":"Rohit Roy","userId":"13123088003246438727"}},"outputId":"53fb3f62-1d2d-45da-c8a2-e0e731b33aad"},"outputs":[{"output_type":"stream","name":"stdout","text":["The sentences generated are :\n"]},{"output_type":"execute_result","data":{"text/plain":["['for his Canadian landscape scenes . Paul Southern ( born',\n"," 'hockey players : Dillon played in the Greater Brisbane League',\n"," 'by Mad Man Pondo and Necro Butcher . The first',\n"," 'cartoon short , directed by Dr. Balakrishnan and produced by',\n"," 'history to play in the West Godavari district of the']"]},"metadata":{},"execution_count":29}],"source":["# Now to generate the sentences using the 4 gram model.\n","\n","# We use a random start word. So we find all the trigrams that start with that word.\n","def trigram_getter(word):\n","    trigram_list = []\n","    for i in list(ngram_model.keys()):\n","        if word == i[0]:\n","            trigram_list.append(i)\n","    return trigram_list\n","\n","def sent_generate(start_word):\n","    start_trigram = random.choice(trigram_getter(start_word))\n","    list_sent = list(start_trigram)\n","    for i in range(7):\n","        next_word_prob = ngram_model[tuple(list_sent[-3:])]\n","        if not next_word_prob:\n","            break\n","        next_word = random.choices(list(next_word_prob.keys()),list(next_word_prob.values()))[0]\n","        list_sent.append(next_word)\n","    sentence = \" \".join(list_sent)\n","    return sentence\n","\n","# Generate 5 sentences using random trigrams as starting points\n","gen_sent = []\n","\n","for i in range(5):\n","    random_start_word = random.choice(words)\n","    sentence = sent_generate(random_start_word)\n","    gen_sent.append(sentence)\n","\n","print(\"The sentences generated are :\")\n","gen_sent"]},{"cell_type":"markdown","metadata":{"id":"CAw3f7VsmWiq"},"source":["# Part 2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cL8Ng4WhmWiq"},"outputs":[],"source":["# Writing the entire corpus in POS, saving this to a file called pos_wiki.txt\n","\n","tagged_corpus = []\n","\n","with open(\"pos_wiki.txt\",\"w\") as file:\n","\n","    for sent in sentences:\n","        words = word_tokenize(sent)\n","        tagged_words = pos_tag(words)\n","        tagged_corpus.append(tagged_words)\n","        #print(tagged_words)\n","        for word in tagged_words:\n","            file.write(f\"{word[1]} \")\n","        file.write(\"\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wPyzcXYqmWir"},"outputs":[],"source":["# Finding frequency of each structure.\n","\n","from collections import Counter\n","\n","pos_counter = {}\n","\n","with open(\"pos_wiki.txt\",\"r\") as file:\n","    for sent in file.readlines():\n","        sent = sent.strip()\n","        if sent in pos_counter.keys():\n","            pos_counter[sent] += 1\n","        else:\n","            pos_counter[sent] = 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V9rzVlnlmWir","outputId":"316170a2-19af-48df-b222-cefcd5ddd151"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of unique POS : 6385780\n","Top 5 structures are :\n","['NNP NN VBZ DT NNS IN NN IN DT NN NNP .', 'NNP NN VBZ DT NN IN DT NN NNP .', 'NNP NN VBZ DT NN IN DT NNP NN .', 'NNP NN VBZ DT NNS IN NN IN DT NNP NN .', 'NNP VBZ DT NN IN NNS IN DT NN NNP .']\n"]}],"source":["# Number of unique structures.\n","\n","print(f\"Number of unique POS : {len(pos_counter)}\")\n","\n","# Sort the dictionary items by values in descending order and get the top 5 keys.\n","top_pos = sorted(pos_counter, key=lambda x: pos_counter[x], reverse=True)[:5]\n","\n","print(\"Top 5 structures are :\")\n","print(top_pos)"]},{"cell_type":"markdown","metadata":{"id":"T7OPo9PhmWir"},"source":["# Part 3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KsCnCdRUmWir"},"outputs":[],"source":["# Making a nested dictionary of POS structures.\n","\n","pos_dict = {}\n","\n","for sent in tagged_corpus:\n","    for key, value in sent:\n","        if value in pos_dict.keys():\n","            if key in (pos_dict[value]).keys():\n","                pos_dict[value][key]+=1\n","            else:\n","                pos_dict[value][key]=1\n","        else:\n","            pos_dict[value] = {key: 1}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V8gI2vUSmWis","outputId":"2c2ebb6e-6c7a-46db-871d-cf2c4d586a59"},"outputs":[{"name":"stdout","output_type":"stream","text":["Structure 1:\n","NNP NN VBZ DT NNS IN NN IN DT NN NNP .\n","\n"," 1 . Southwest insectivore is the stories of planning of the city Santa . \n"," 2 . Frank cell is a donors of passenger of the nomenclature College . \n"," 3 . January rodent has the dependents of dash in a album Works . \n"," 4 . Committee album is the crowds through harmony from The list Australia . \n"," 5 . Policy reality is a people for moth by the appeal Comedy . \n","\n","Structure 2:\n","NNP NN VBZ DT NN IN DT NN NNP .\n","\n"," 1 . Stabber team chooses This steel of the practice Munich . \n"," 2 . Essex hurdler is the production at the program State . \n"," 3 . Lough heritage plays the foliage In the photographs Lake . \n"," 4 . Vaine constituency is the component in the midday Computer . \n"," 5 . Office musician is The section from a place Ronald . \n","\n","Structure 3:\n","NNP NN VBZ DT NN IN DT NNP NN .\n","\n"," 1 . City population plays a logo in the A. county . \n"," 2 . Jersey plank is the instruction in an Punjab research . \n"," 3 . Invest case is an series in the Nigerian dementia . \n"," 4 . Michael mascot is the process while a Park Order . \n"," 5 . Conservation son finds the music of the Place player . \n","\n","Structure 4:\n","NNP NN VBZ DT NNS IN NN IN DT NNP NN .\n","\n"," 1 . Arts season appears the rivers from arrangement by a Rugby won . \n"," 2 . School energy contains the species in loyalty for the Devji village . \n"," 3 . September album has the switches in Today of a Fraley preparation . \n"," 4 . Index story has The slaves In heyday from The Kyrgyzstani purpose . \n"," 5 . Civic report makes a individuals on rider of the Rising aide-de-camp . \n","\n","Structure 5:\n","NNP VBZ DT NN IN NNS IN DT NN NNP .\n","\n"," 1 . Leaks works a film as infants upon a bishop Appeal . \n"," 2 . Thackley is an bicolor in services of a gastropod Scotland . \n"," 3 . Pal is the release of cells by the skater Parliament . \n"," 4 . Lake is the crime as forests by a film Lesotho . \n"," 5 . Sergio does The modulator about authors among a history Orleton . \n","\n"]}],"source":["# Making probability distribution for each POS, and generating sentences for top_pos.\n","\n","pos_prob_dict={}\n","\n","for x in pos_dict.keys():\n","    pos_prob_dict[x]=dict(sorted((pos_dict[x]).items(), key=lambda x:x[1], reverse=True))\n","\n","for w in pos_prob_dict:\n","    total_count = sum(pos_prob_dict[w].values())\n","    for word in pos_prob_dict[w]:\n","        pos_prob_dict[w][word] = pos_prob_dict[w][word] / total_count\n","\n","random.seed(17)\n","\n","for i in range(len(top_pos)):\n","    print(f\"Structure {i+1}:\")\n","    print(top_pos[i])\n","    for j in range(5):\n","        print(f\"\\n {j+1} .\", end = \" \")\n","        for j in range(len(list(top_pos[i].split()))):\n","            pos_list = list(top_pos[i].split())\n","            next_word = random.choices(list(pos_prob_dict[pos_list[j]].keys()), weights=list(pos_prob_dict[pos_list[j]].values()), k=1)\n","            print(next_word[0], end=\" \")\n","    print(\"\\n\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}