{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59VkH2ELCXqh"
      },
      "source": [
        "# NLP : Assignment 3\n",
        "### Rohit Roy | MDS202340"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fqeB8S_yCXqj"
      },
      "outputs": [],
      "source": [
        "from collections import Counter, defaultdict\n",
        "from wiki_dump_reader import Cleaner , iterate\n",
        "\n",
        " \n",
        "from nltk import ngrams, word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from scipy import sparse\n",
        "from scipy.sparse import dok_matrix\n",
        "from scipy.spatial.distance import cosine\n",
        "from scipy.sparse.linalg import svds\n",
        "\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.font_manager as fm\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "corpus_file = 'HindiCorpus.txt'\n",
        "preprocessed_file='ProcessedCorpus.txt' \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "MInimize corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def minimize_pre(so,de):\n",
        "\twith open(so, 'r',encoding=\"utf-8\") as sor:\n",
        "\t\twith open(de, 'w',encoding=\"utf-8\") as des:\n",
        "\t\t\t\tfor line in sor:\n",
        "\t\t\t\t\tdes.write(line[:(len(line)//4)])\n",
        "\t\n",
        "minimize_pre('ProcessedCorpus-Org.txt','ProcessedCorpus.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xaM4HnQCXqm"
      },
      "source": [
        "#### 1. Building Vocabulary\n",
        "\n",
        "I am using the Bengali corpus from the 1st assignment, which was split into 8 files each of 100000 lines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "K9_i3E9-CXqm"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'Cleaner' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 18\u001b[0m\n\u001b[0;32m     14\u001b[0m \t\toutput\u001b[38;5;241m.\u001b[39mclose ()\n\u001b[0;32m     15\u001b[0m \t\u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mpage count = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m \u001b[43mwrite_corpus\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[1], line 4\u001b[0m, in \u001b[0;36mwrite_corpus\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrite_corpus\u001b[39m ():\n\u001b[0;32m      3\u001b[0m \tpage_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 4\u001b[0m \tcleaner \u001b[38;5;241m=\u001b[39m \u001b[43mCleaner\u001b[49m ()\n\u001b[0;32m      5\u001b[0m \t\u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(corpus_file , \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf -8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m output:\n\u001b[0;32m      6\u001b[0m \t\t\u001b[38;5;28;01mfor\u001b[39;00m title , text \u001b[38;5;129;01min\u001b[39;00m iterate(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhiwiki-latest-pages-articles.xml\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
            "\u001b[1;31mNameError\u001b[0m: name 'Cleaner' is not defined"
          ]
        }
      ],
      "source": [
        "def write_corpus ():\n",
        "\t\n",
        "\tpage_count = 0\n",
        "\tcleaner = Cleaner ()\n",
        "\twith open(corpus_file , 'w', encoding='utf -8') as output:\n",
        "\t\tfor title , text in iterate('hiwiki-latest-pages-articles.xml'):\n",
        "\t\t\ttext = cleaner.clean_text(text)\n",
        "\t\t\tcleaned_text , _ = cleaner.build_links(text)\n",
        "\t\t\toutput.write(title + '\\n' + cleaned_text + '\\n ')\n",
        "\t\t\tpage_count += 1\n",
        "\t\t\tif page_count % 1000 == 0:\n",
        "\t\t\t\tprint(f'Pages dumped = {page_count}', end='\\r')\n",
        "\t\t\t\t#print(title + '\\n' + cleaned_text + '\\n')\n",
        "\t\toutput.close ()\n",
        "\tprint(f\"\\npage count = {page_count}\")\n",
        "\n",
        "\n",
        "write_corpus ()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lines processed = 4232000\r"
          ]
        }
      ],
      "source": [
        "\n",
        "import string\n",
        "\n",
        "stop_words=[]\n",
        "\n",
        "start = int('0x0900', 16)\n",
        "end = int('0x097F', 16)\n",
        "\n",
        "num_start = int('0x0966', 16)\n",
        "num_end = int('0x096F', 16)\n",
        "\n",
        "hindi_letters = [chr(code) for code in range(start, end + 1)]\n",
        "hindi_letters.remove('рее')\n",
        "hindi_letters.remove('ред')\n",
        "hindi_letters.remove('ре╜')\n",
        "\n",
        "hindi_numbers = [chr(code) for code in range(num_start, num_end + 1)]\n",
        "\n",
        "def create_stop_words(file_path):\n",
        "\ttry :\n",
        "\t\twith open(file_path, 'r',encoding=\"utf-8\") as file:\n",
        "\t\t\tfor line in file:\n",
        "\t\t\t\tstop_words.append(line.strip())\n",
        "\texcept:\n",
        "\t\tprint()\n",
        "\n",
        "def remove_numbers(text):\n",
        "\ttokens = list(text)\n",
        "\tfiltered_tokens = []\n",
        "\n",
        "\tfor token in tokens:\n",
        "\t\tif token not in hindi_numbers:\n",
        "\t\t\tfiltered_tokens.append(token)\n",
        "\n",
        "\tresult = ''.join(filtered_tokens)\n",
        "\treturn result\n",
        "\n",
        "def remove_punctuation(text):\n",
        "\ttranslator = str.maketrans('', '', string.punctuation)\n",
        "\treturn text.translate(translator)\n",
        "\n",
        "def remove_whitespace(text):\n",
        "\treturn \" \".join(text.split())\n",
        "\t\t\t\n",
        "file_path = 'stopwords.txt'  # Replace 'example.txt' with the path to your file\n",
        "create_stop_words(file_path)\n",
        "\n",
        "\n",
        "def preprocess_line(text):\n",
        "\ttext=remove_whitespace(remove_punctuation(remove_numbers(text)))\n",
        "\t\n",
        "\twords = text.split(\" \")\n",
        "\n",
        "\t# Filter out foreign words\n",
        "\tfiltered_tokens = []\n",
        "\tfor word in words:\n",
        "\t\tif word in stop_words:\n",
        "\t\t\tcontinue\n",
        "\t\t\n",
        "\t\tbreaker=False\n",
        "\t\tfor letter in word:\n",
        "\t\t\tif letter not in hindi_letters:\n",
        "\t\t\t\tbreaker=True\n",
        "\t\t\t\tbreak\n",
        "\t\t\n",
        "\t\tif breaker:\n",
        "\t\t\tcontinue\n",
        "\t\t\t\n",
        "\t\t\t\n",
        "\t\tfiltered_tokens.append(word)\n",
        "\t\n",
        "\t# Reassemble the text\n",
        "\tprocessed_text = ' '.join(filtered_tokens)\n",
        "\t\n",
        "\treturn processed_text\n",
        "\n",
        "\n",
        "def preprocess_corpus(source_file, destination_file):\n",
        "\tpage_count=0\n",
        "\twith open(source_file, 'r',encoding=\"utf-8\") as source:\n",
        "\t\twith open(destination_file, 'w',encoding=\"utf-8\") as destination:\n",
        "\t\t\tfor line in source:\n",
        "\t\t\t\tdestination.write(preprocess_line(line))\n",
        "\t\t\t\tpage_count+=1\n",
        "\t\t\t\tif page_count % 1000 == 0:\n",
        "\t\t\t\t\tprint(f'Lines processed = {page_count}', end='\\r')\n",
        "\n",
        "# Example usage\n",
        "preprocessed_file='ProcessedCorpus.txt' \n",
        "source_file_path = corpus_file  # Replace 'source.txt' with the path to your source file\n",
        "destination_file_path = preprocessed_file  # Replace 'destination.txt' with the path to your destination file\n",
        "\n",
        "preprocess_corpus(source_file_path, destination_file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Tf2ISsKfCXqn"
      },
      "outputs": [],
      "source": [
        "\n",
        "def gen_token_count(file):\n",
        "\ttoken_len=0\n",
        "\twith open(file, 'r',encoding=\"utf-8\") as f:\n",
        "\t\tfor line in f:\n",
        "\t\t\tspace_count=0\n",
        "\t\t\tfor char in line:\n",
        "\t\t\t\tif char == ' ':\n",
        "\t\t\t\t\t\tspace_count += 1\n",
        "\n",
        "\t\t\ttoken_len+=space_count+1\n",
        "\t\t\t\n",
        "\treturn token_len\n",
        "\n",
        "def gen_token(file):\n",
        "\ttokens=[]\n",
        "\t\n",
        "\twith open(file, 'r',encoding=\"utf-8\") as f:\n",
        "\t\tfor line in f:\n",
        "\t\t\ttokens+=(line.split(' '))\n",
        "\t\t\t\n",
        "\treturn tokens\n",
        "\n",
        "def gen_vocabulary(file):\n",
        "\tword_list=[]\n",
        "\twith open(file, 'r',encoding=\"utf-8\") as f:\n",
        "\t\tfor line in f:\n",
        "\t\t\tword_list+=(line.split(' '))\n",
        "\t\t\t\n",
        "\tword_counts = Counter(word_list)\n",
        "\treturn word_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9CjVMVMCCXqn",
        "outputId": "85b84b6f-e813-46a4-b688-c43a63a4312a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of Corpus Tokens : 86699267\n",
            "Size of Pre Processed Corpus Tokens : 10575437\n"
          ]
        }
      ],
      "source": [
        "corpus_token_count=gen_token_count(corpus_file)\n",
        "print(\"Size of Corpus Tokens :\",corpus_token_count)\n",
        "\n",
        "preprocess_token_count=gen_token_count(preprocessed_file)\n",
        "print(\"Size of Pre Processed Corpus Tokens :\",preprocess_token_count)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of vocab : 863935\n"
          ]
        }
      ],
      "source": [
        "vocab_counts = gen_vocabulary(preprocessed_file)\n",
        "print(\"Size of vocab :\",len(vocab_counts))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of reduced vocab : 9270\n"
          ]
        }
      ],
      "source": [
        "\n",
        "vocab = {x for x, count in vocab_counts.items() if count >= 100}\n",
        "print(\"Size of reduced vocab :\", len(vocab))\n",
        "\n",
        "\n",
        "# Number to word\n",
        "vocab_list = list(vocab)\n",
        "\n",
        "# Corresponding word to number\n",
        "vocab_pos = {vocab_list[i] : i for i in range(len(vocab_list))}\n",
        "\n",
        "# Compiling both into a dictionary\n",
        "vocab_idx = vocab_pos.copy()\n",
        "vocab_idx.update({i : w for i,w in enumerate(vocab_list)})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2A1g-mycCXqq"
      },
      "source": [
        "#### 2. Building Co-occurance matrix\n",
        "\n",
        "The function below returns a co-occurance matrix, which is a csr matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def co_occurances(file):\n",
        "\twindow = 5\n",
        "\n",
        "\t# Using a ramp of window 4\n",
        "\tramp = [0] + [*range(window,0,-1)]     \n",
        "\n",
        "\toccurances = defaultdict(lambda : Counter())\n",
        "\n",
        "\twith open(file, 'r',encoding=\"utf-8\") as corpus:\n",
        "\t\tfor line in corpus:\n",
        "\t\t\tall_grams = ngrams(word_tokenize(line), window+1, pad_right = True, pad_left = True)\n",
        "\t\t\tfor grams in all_grams:\n",
        "\n",
        "\t\t\t\tif grams[0] in vocab :\n",
        "\t\t\t\t\tfor idx, gram in enumerate(grams):\n",
        "\t\t\t\t\t\tif gram in vocab:\n",
        "\t\t\t\t\t\t\toccurances[vocab_idx[grams[0]]][vocab_idx[gram]] += ramp[idx]\n",
        "\n",
        "\t\t\t\t# Doing the same as above with the gram reversed\n",
        "\t\t\t\tgrams_rev = grams[::-1]\n",
        "\n",
        "\t\t\t\tif grams_rev[0] in vocab:\n",
        "\t\t\t\t\tfor idx, gram in enumerate(grams_rev):\n",
        "\t\t\t\t\t\tif gram in vocab:\n",
        "\t\t\t\t\t\t\toccurances[vocab_idx[grams_rev[0]]][vocab_idx[gram]] += ramp[idx]\n",
        "\n",
        "\t# Initializing a 0 matrix\n",
        "\tmat = np.zeros((len(vocab), len(vocab)), dtype=np.int64)\n",
        "\n",
        "\t# Filling the matrix\n",
        "\tfor i in tqdm(range(len(vocab))):\n",
        "\t\tfor j in occurances[i].keys():\n",
        "\t\t\tmat[i][j] = occurances[i][j]\n",
        "\n",
        "\treturn mat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "kF1x_PzfCXqr",
        "outputId": "ab197e51-5b63-4604-9a05-8108ab5719f2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 9270/9270 [00:01<00:00, 4803.41it/s]\n"
          ]
        }
      ],
      "source": [
        "cocrmat = co_occurances(preprocessed_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def calculate_probability(co_occurrence_matrix):\n",
        "\t# Calculate row sums and column sums\n",
        "\trow_sums = np.sum(co_occurrence_matrix, axis=1)\n",
        "\tcol_sums = np.sum(co_occurrence_matrix, axis=0)\n",
        "\t\n",
        "\t# Total co-occurrences\n",
        "\tN = np.sum(co_occurrence_matrix)\n",
        "\t\n",
        "\t# Calculate pi and pj\n",
        "\tpi = row_sums / N\n",
        "\tpj = col_sums / N\n",
        "\t\n",
        "\treturn pi, pj\n",
        "# Calculate pi and pj\n",
        "pi, pj = calculate_probability(cocrmat)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 9270/9270 [00:34<00:00, 265.65it/s]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def gen_ppmi_matrix(co_occurrence_matrix):\n",
        "\tppmi_matrix=np.zeros (( len(vocab) , len(vocab)))\n",
        "\n",
        "\trow_sums = np.sum(co_occurrence_matrix, axis=1)\n",
        "\tfor\ti in tqdm(range(len(vocab))):\n",
        "\t\tfor j in range(len(vocab)):\n",
        "\n",
        "\t\t\tif(row_sums[i]==0 or co_occurrence_matrix[i][j]==0):\n",
        "\t\t\t\tppmi_matrix[i][j]=0\n",
        "\t\t\t\tcontinue\n",
        "\t\t\tpij=((co_occurrence_matrix[i][j]))/row_sums[i]\n",
        "\t\t\tppmi_matrix[i][j]=max(0,math.log2(pij/(pi[i]*pj[j])))\n",
        "\n",
        "\treturn ppmi_matrix\n",
        "\n",
        "ppmi_matrix=gen_ppmi_matrix(cocrmat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "23.301875711325586\n",
            "<class 'numpy.float64'>\n"
          ]
        }
      ],
      "source": [
        "print((ppmi_matrix[0][0]))\n",
        "print(type(ppmi_matrix[0][0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 9270/9270 [01:29<00:00, 103.66it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "count =0\n",
        "for i in tqdm(range(len(ppmi_matrix))):\n",
        "\tfor j in range(len(ppmi_matrix)):\n",
        "\t\tif(ppmi_matrix[i][j]==math.inf):\n",
        "\t\t\tcount+=1\n",
        "print(count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_rYr-TE2-ta"
      },
      "source": [
        "#### 3. Building Word-embedding matrix\n",
        "\n",
        "We iterate through the non-zero entries of the co-occurance matrix and calculate the correlation as per the formula given in the COALS paper. We don't care about the 0 entries since it is going to give us negative correlation anyway."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 15 popular words :  ['рдХреЗ', 'рд░рд╛рдЬреНрдп', 'рд░реВрдк', 'рдирд╣реАрдВ', 'рдкреНрд░рджреЗрд╢', 'рдирд╛рдо', 'рдШреА', 'рднрд╛рд░рдд', 'рдЕрдзрд┐рдХ', 'рдХреНрд╖реЗрддреНрд░', 'рд╕реНрдерд┐рдд', 'рдирд┐рд░реНрдорд╛рдг', 'рд╕рдмрд╕реЗ', 'рдЬрд┐рд▓реЗ', 'рджрдХреНрд╖рд┐рдг']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "top_15 = vocab_counts.most_common(15)\n",
        "top_15=[i for i,_ in top_15]\n",
        "print(\"Top 15 popular words : \", top_15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HSihZE72-tb"
      },
      "source": [
        "#### 4. Finding Similar Words\n",
        "\n",
        "We find the k closest words to the selected word using the closest_words function which also returns the cosine distance between them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "cYwx9_aR2-tb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['рд░рд╛рдЬреНрдп', 'рд░реВрдк', 'рдкреНрд░рджреЗрд╢', 'рдирд╛рдо', 'рдШреА', 'рднрд╛рд░рдд', 'рдХреНрд╖реЗрддреНрд░', 'рдирд┐рд░реНрдорд╛рдг', 'рдЬрд┐рд▓реЗ', 'рджрдХреНрд╖рд┐рдг']\n"
          ]
        }
      ],
      "source": [
        "nouns = [top_15[1],top_15[2],top_15[4],top_15[5],top_15[6],top_15[7],top_15[9],top_15[11],top_15[13],top_15[14]]\n",
        "print(nouns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def calc_norm(vec):\n",
        "\tsum_of_squares = sum(x**2 for x in vec)\n",
        "\treturn math.sqrt(sum_of_squares)\n",
        "\n",
        "def cosine_similarity(vec1, vec2):\n",
        "\tv1=list(vec1)\n",
        "\tv2=list(vec2)\n",
        "\t\n",
        "\tdot_product=(0.0)\n",
        "\n",
        "\tfor i in range(len(v1)):\n",
        "\t\ttemp=dot_product\n",
        "\t\tdot_product+=(v1[i]*v2[i])\n",
        "\t#print(dot_product)\n",
        "\tnorm_vec1 = calc_norm(v1)\n",
        "\tnorm_vec2 = calc_norm(v2)\n",
        "\treturn dot_product / (norm_vec1 * norm_vec2)\n",
        "\n",
        "def reshape_top(words, extra):\n",
        "\tlow = 0\n",
        "\thigh = len(words) - 1\n",
        "\n",
        "\twhile low <= high:\n",
        "\t\tmid = (low + high) // 2\n",
        "\t\tif words[mid][1] < extra[1]:\n",
        "\t\t\tlow = mid + 1\n",
        "\t\telif words[mid][1] > extra[1]:\n",
        "\t\t\thigh = mid - 1\n",
        "\t\telse:\n",
        "\t\t\tbreak\n",
        "\n",
        "\twords.insert(low, extra)\n",
        "\treturn words[1:]\n",
        "\n",
        "def find_similar_words(target_word, vocab, ppmi_matrix, top_n=10):\n",
        "\ttop_n+=1\n",
        "\tif target_word not in vocab:\n",
        "\t\tprint(\"Target word not found in the vocabulary.\")\n",
        "\t\treturn\n",
        "\t\n",
        "\ttarget_index = vocab_pos[target_word]\n",
        "\ttarget_vector = ppmi_matrix[target_index]\n",
        "\n",
        "\tsimilarities = [(vocab_idx[0],0.0) for _ in range(top_n)]\n",
        "\n",
        "\tfor i, vector in enumerate(ppmi_matrix):\n",
        "\t\tsim = cosine_similarity(target_vector, vector)\n",
        "\t\tsimilarities=reshape_top(similarities,((vocab_idx[i], sim)))\n",
        "\n",
        "\tsimilarities.reverse()\n",
        "\n",
        "\treturn similarities[1:]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[61], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m noun \u001b[38;5;129;01min\u001b[39;00m nouns:\n\u001b[0;32m      2\u001b[0m \ttarget_word \u001b[38;5;241m=\u001b[39m noun\n\u001b[1;32m----> 3\u001b[0m \tsimilar_words \u001b[38;5;241m=\u001b[39m \u001b[43mfind_similar_words\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_word\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mppmi_matrix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \t\u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWords similar to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_word\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \t\u001b[38;5;28;01mfor\u001b[39;00m word, similarity \u001b[38;5;129;01min\u001b[39;00m similar_words:\n",
            "Cell \u001b[1;32mIn[49], line 49\u001b[0m, in \u001b[0;36mfind_similar_words\u001b[1;34m(target_word, vocab, ppmi_matrix, top_n)\u001b[0m\n\u001b[0;32m     46\u001b[0m similarities \u001b[38;5;241m=\u001b[39m [(vocab_idx[\u001b[38;5;241m0\u001b[39m],\u001b[38;5;241m0.0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(top_n)]\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, vector \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(ppmi_matrix):\n\u001b[1;32m---> 49\u001b[0m \tsim \u001b[38;5;241m=\u001b[39m \u001b[43mcosine_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_vector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvector\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m \tsimilarities\u001b[38;5;241m=\u001b[39mreshape_top(similarities,((vocab_idx[i], sim)))\n\u001b[0;32m     52\u001b[0m similarities\u001b[38;5;241m.\u001b[39mreverse()\n",
            "Cell \u001b[1;32mIn[49], line 18\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[1;34m(vec1, vec2)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m#print(dot_product)\u001b[39;00m\n\u001b[0;32m     17\u001b[0m norm_vec1 \u001b[38;5;241m=\u001b[39m calc_norm(v1)\n\u001b[1;32m---> 18\u001b[0m norm_vec2 \u001b[38;5;241m=\u001b[39m \u001b[43mcalc_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dot_product \u001b[38;5;241m/\u001b[39m (norm_vec1 \u001b[38;5;241m*\u001b[39m norm_vec2)\n",
            "Cell \u001b[1;32mIn[49], line 4\u001b[0m, in \u001b[0;36mcalc_norm\u001b[1;34m(vec)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalc_norm\u001b[39m(vec):\n\u001b[1;32m----> 4\u001b[0m \tsum_of_squares \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(x\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m vec)\n\u001b[0;32m      5\u001b[0m \t\u001b[38;5;28;01mreturn\u001b[39;00m math\u001b[38;5;241m.\u001b[39msqrt(sum_of_squares)\n",
            "Cell \u001b[1;32mIn[49], line 4\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalc_norm\u001b[39m(vec):\n\u001b[1;32m----> 4\u001b[0m \tsum_of_squares \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(x\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m vec)\n\u001b[0;32m      5\u001b[0m \t\u001b[38;5;28;01mreturn\u001b[39;00m math\u001b[38;5;241m.\u001b[39msqrt(sum_of_squares)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "for noun in nouns:\n",
        "\ttarget_word = noun\n",
        "\tsimilar_words = find_similar_words(target_word, vocab, ppmi_matrix)\n",
        "\n",
        "\tprint(f\"Words similar to '{target_word}':\")\n",
        "\tfor word, similarity in similar_words:\n",
        "\t\tprint(f\"\t{word}: {similarity}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
